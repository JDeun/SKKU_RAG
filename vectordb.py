"""
VectorDB ìƒì„± ëª¨ë“ˆ
=================
PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , C-P-P(Composition-Process-Property)ë¥¼ ì¶”ì¶œí•˜ì—¬
ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ VectorDBì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import warnings

# ë¡œê·¸ ì–µì œ ì„¤ì • (imports ì „ì— ì‹¤í–‰)
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # TensorFlow ë¡œê·¸ ì–µì œ
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # oneDNN ë©”ì‹œì§€ ì–µì œ

from pathlib import Path
from typing import List, Optional
import tiktoken
from tqdm import tqdm

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (pymupdf ì‚¬ìš©)
import fitz  # PyMuPDF

# ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
import config
import prompts


# ==================== í† í° ê³„ì‚° ìœ í‹¸ë¦¬í‹° ====================
tokenizer = tiktoken.get_encoding("cl100k_base")

def tiktoken_len(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        text: í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        
    Returns:
        í† í° ê°œìˆ˜
    """
    tokens = tokenizer.encode(text)
    return len(tokens)


# ==================== PDF ë¡œë“œ ====================
def load_single_pdf(filepath: str, filename: str) -> List[Document]:
    """
    ë‹¨ì¼ PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    PyMuPDF(fitz)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        filepath: PDF íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        filename: PDF íŒŒì¼ëª… (ë©”íƒ€ë°ì´í„°ìš©)
        
    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (ê° í˜ì´ì§€ë³„ë¡œ)
    """
    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(filepath):
            print(f"âš ï¸  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")
            return []
        
        # PyMuPDFë¡œ PDF ì—´ê¸°
        doc = fitz.open(filepath)
        
        # ì•”í˜¸í™” í™•ì¸
        if doc.is_encrypted:
            print(f"ğŸ”’ ì•”í˜¸í™”ëœ PDF ê±´ë„ˆëœ€: {filename}")
            doc.close()
            return []
        
        total_pages = len(doc)
        pages_with_metadata = []
        
        # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for page_num in range(total_pages):
            page = doc[page_num]
            text = page.get_text("text")  # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            
            # ë¹ˆ í˜ì´ì§€ ìŠ¤í‚µ
            if not text.strip():
                continue
            
            pages_with_metadata.append(
                Document(
                    page_content=text,
                    metadata={
                        'source': filename,
                        'page': page_num + 1,  # 1ë¶€í„° ì‹œì‘
                        'total_pages': total_pages
                    }
                )
            )
        
        doc.close()
        print(f"  âœ“ {filename}: {len(pages_with_metadata)} í˜ì´ì§€ ë¡œë“œ")
        return pages_with_metadata
        
    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {filename} - {e}")
        return []


def load_pdfs(pdf_path: str) -> List[Document]:
    """
    PDF íŒŒì¼ ë˜ëŠ” í´ë”ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ ë˜ëŠ” í´ë” ê²½ë¡œ
        
    Returns:
        ëª¨ë“  í˜ì´ì§€ì˜ Document ë¦¬ìŠ¤íŠ¸
    """
    pdf_path = Path(pdf_path)
    all_pages = []
    
    if not pdf_path.exists():
        print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")
        return []
    
    # í´ë”ì¸ ê²½ìš°
    if pdf_path.is_dir():
        pdf_files = list(pdf_path.glob("*.pdf"))
        if not pdf_files:
            print("âš ï¸  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ“‚ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬")
        for pdf_file in tqdm(pdf_files, desc="PDF ë¡œë“œ ì¤‘"):
            pages = load_single_pdf(str(pdf_file), pdf_file.name)
            all_pages.extend(pages)
    
    # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°
    elif pdf_path.is_file() and pdf_path.suffix.lower() == '.pdf':
        print(f"ğŸ“„ ë‹¨ì¼ PDF íŒŒì¼ ë¡œë“œ: {pdf_path.name}")
        pages = load_single_pdf(str(pdf_path), pdf_path.name)
        all_pages.extend(pages)
    
    else:
        print(f"âŒ ìœ íš¨í•œ PDF íŒŒì¼ ë˜ëŠ” í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {pdf_path}")
    
    print(f"âœ… ì´ {len(all_pages)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n")
    return all_pages


# ==================== í…ìŠ¤íŠ¸ ë¶„í•  ====================
def split_documents(
    documents: List[Document],
    chunk_size: int = config.CHUNK_SIZE,
    chunk_overlap: int = config.CHUNK_OVERLAP
) -> List[Document]:
    """
    ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    Args:
        documents: ë¶„í• í•  Document ë¦¬ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸° (í† í° ìˆ˜)
        chunk_overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
        
    Returns:
        ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if not documents:
        print("âš ï¸  ë¶„í• í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ“„ ë¬¸ì„œ ë¶„í•  ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì˜¤ë²„ë©: {chunk_overlap})...")
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=tiktoken_len,
        separators=["\n\n", "\n", " ", ""]
    )
    
    chunks = splitter.split_documents(documents)
    
    # ì¤‘ë³µ ì œê±° (ë™ì¼ ë‚´ìš© + ë™ì¼ ì¶œì²˜ + ë™ì¼ í˜ì´ì§€)
    unique_chunks = []
    seen = set()
    for chunk in chunks:
        key = (
            chunk.page_content.strip(),
            chunk.metadata.get("source", ""),
            chunk.metadata.get("page", None)
        )
        if key not in seen:
            seen.add(key)
            unique_chunks.append(chunk)
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± â†’ ì¤‘ë³µ ì œê±° í›„ {len(unique_chunks)}ê°œ\n")
    return unique_chunks


def add_cpp_to_chunks(
    chunks: List[Document],
    batch_size: int = 5
) -> List[Document]:
    """
    ëª¨ë“  ì²­í¬ì— C-P-P ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    JSON Output Parserë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸° (í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜)
        
    Returns:
        C-P-P ë©”íƒ€ë°ì´í„°ê°€ ì¶”ê°€ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if not chunks:
        return []
    
    print(f"ğŸ”¬ C-P-P ì¶”ì¶œ ì¤‘ (ì´ {len(chunks)}ê°œ ì²­í¬)...")
    
    # LLM ë° ì¶”ì¶œ ì²´ì¸ ì´ˆê¸°í™”
    llm = ChatGoogleGenerativeAI(
        model=config.LLM_MODEL_NAME,
        temperature=config.LLM_TEMPERATURE,
        google_api_key=config.GOOGLE_API_KEY
    )
    extraction_chain = prompts.CPP_EXTRACTION_PROMPT | llm | prompts.json_parser
    
    processed_chunks = []
    for i in tqdm(range(0, len(chunks)), desc="C-P-P ì¶”ì¶œ"):
        chunk = chunks[i]
        try:
            # ì²´ì¸ ì‹¤í–‰
            cpp = extraction_chain.invoke({"text": chunk.page_content})
        except Exception as e:
            # íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            print(f"âš ï¸  C-P-P ì¶”ì¶œ/íŒŒì‹± ì˜¤ë¥˜ (ì²­í¬ {i}): {e}")
            cpp = {"composition": "N/A", "process": "N/A", "property": "N/A"}
            
        # ë©”íƒ€ë°ì´í„°ì— C-P-P ì¶”ê°€
        chunk.metadata.update(cpp)
        processed_chunks.append(chunk)
    
    print(f"âœ… C-P-P ì¶”ì¶œ ì™„ë£Œ\n")
    return processed_chunks


# ==================== VectorDB ìƒì„±/ë¡œë“œ ====================
def create_or_load_vectordb(
    chunks: Optional[List[Document]] = None,
    persist_directory: str = str(config.VECTOR_DB_PATH),
    force_recreate: bool = False
) -> Chroma:
    """
    VectorDBë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        chunks: ì €ì¥í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ì¡´ DB ë¡œë“œ)
        persist_directory: DB ì €ì¥ ê²½ë¡œ
        force_recreate: Trueì´ë©´ ê¸°ì¡´ DB ì‚­ì œ í›„ ì¬ìƒì„±
        
    Returns:
        Chroma VectorDB ì¸ìŠ¤í„´ìŠ¤
    """
    # Embedding ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={"device": config.EMBEDDING_DEVICE}
    )
    
    # ê¸°ì¡´ DBê°€ ìˆê³  ì¬ìƒì„±ì´ ì•„ë‹Œ ê²½ìš°
    if os.path.exists(persist_directory) and not force_recreate:
        print(f"ğŸ“‚ ê¸°ì¡´ VectorDB ë¡œë“œ: {persist_directory}")
        try:
            db = Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
            )
            print(f"âœ… VectorDB ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {db._collection.count()})\n")
            return db
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ìƒˆë¡œìš´ DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n")
    
    # ìƒˆ DB ìƒì„±
    if chunks is None or len(chunks) == 0:
        print("âŒ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"ğŸ’¾ VectorDB ìƒì„± ì¤‘ ({len(chunks)}ê°œ ì²­í¬)...")
    try:
        db = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        db.persist()
        print(f"âœ… VectorDB ìƒì„± ì™„ë£Œ: {persist_directory}\n")
        return db
    except Exception as e:
        print(f"âŒ VectorDB ìƒì„± ì‹¤íŒ¨: {e}")
        return None


# ==================== ì „ì²´ íŒŒì´í”„ë¼ì¸ ====================
def build_vectordb_pipeline(
    pdf_path: str,
    extract_cpp: bool = True,
    force_recreate: bool = False
) -> Chroma:
    """
    PDF â†’ ì²­í¬ â†’ C-P-P ì¶”ì¶œ â†’ VectorDB ìƒì„±ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸
    
    Args:
        pdf_path: PDF íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ
        extract_cpp: C-P-Pë¥¼ ì¶”ì¶œí• ì§€ ì—¬ë¶€
        force_recreate: ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ì¬ìƒì„±í• ì§€
        
    Returns:
        VectorDB ì¸ìŠ¤í„´ìŠ¤
    """
    print("="*60)
    print("VectorDB êµ¬ì¶• ì‹œì‘")
    print("="*60 + "\n")
    
    # 1. PDF ë¡œë“œ
    documents = load_pdfs(pdf_path)
    if not documents:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 2. ë¬¸ì„œ ë¶„í• 
    chunks = split_documents(documents)
    if not chunks:
        print("âŒ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 3. C-P-P ì¶”ì¶œ (ì˜µì…˜)
    if extract_cpp:
        chunks = add_cpp_to_chunks(chunks)
    
    # 4. VectorDB ìƒì„±
    db = create_or_load_vectordb(
        chunks=chunks,
        force_recreate=force_recreate
    )
    
    print("="*60)
    print("VectorDB êµ¬ì¶• ì™„ë£Œ")
    print("="*60 + "\n")
    
    return db


# ==================== í…ŒìŠ¤íŠ¸ ì½”ë“œ ====================
if __name__ == "__main__":
    # ì„¤ì • ì¶œë ¥
    config.print_config()
    
    # PDF ê²½ë¡œ ì…ë ¥
    pdf_path = input("PDF íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip().strip('"\'')
    
    # VectorDB êµ¬ì¶•
    db = build_vectordb_pipeline(
        pdf_path=pdf_path,
        extract_cpp=True,
        force_recreate=False
    )
    
    if db:
        print("\nâœ… VectorDBê°€ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        test_query = "Cu alloy properties"
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{test_query}'")
        results = db.similarity_search(test_query, k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\n--- ê²°ê³¼ {i} ---")
            print(f"ì¶œì²˜: {doc.metadata.get('source')} (p.{doc.metadata.get('page')})")
            print(f"Composition: {doc.metadata.get('composition', 'N/A')}")
            print(f"Process: {doc.metadata.get('process', 'N/A')[:100]}...")
            print(f"Property: {doc.metadata.get('property', 'N/A')[:100]}...")
